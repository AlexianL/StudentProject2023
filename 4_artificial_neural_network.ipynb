{"cells":[{"cell_type":"markdown","metadata":{"id":"DFlFxDnNRz99"},"source":["# Artificial Neural Network\n","\n","We remind that if the user is using Google Colaboratory, he must change the execution type to GPU, see the README."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yj6XPRdcjWfi"},"outputs":[],"source":["#Libraries for data processing\n","import numpy as np \n","import pandas as pd\n","\n","#Libraries for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns \n","sns.set(color_codes = True)\n","sns.set(font_scale=1.5) #fixing font size\n","\n","#Libraries for artificial neural network\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Activation, Dense, Normalization\n","from keras.optimizers import Adam\n","from keras.metrics import categorical_crossentropy\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.callbacks import Callback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEfwYSFJjWfw"},"outputs":[],"source":["from logging import RootLogger\n","# Mount Google Drive\n","from google.colab import drive #import drive from google colab\n","\n","root = \"/content/drive\"     #default location for the drive\n","\n","drive.mount(root)           #we mount the google drive at /content/drive\n","\n","import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n","from os.path import join  \n","\n","# path to your project on Google Drive\n","my_google_drive_path = \"MyDrive/StudentProject2023\"\n","\n","project_path = join(root, my_google_drive_path)"]},{"cell_type":"markdown","metadata":{"id":"inH-yOIMRz-N"},"source":["##### We import our training data (all nuclei except Z=10,38,54,68,82) and create rescaled inputs based on the dataframe columns for the ANN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u2j7VBytShWW"},"outputs":[],"source":["train_data = pd.read_csv(join(project_path,\"rescaled_data/train_rescaled_data.csv\"), sep=\";\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9eydplDjWf0"},"outputs":[],"source":["#First inputs\n","target = train_data[\"rescaled_ame_BE\"]\n","n_input = train_data[\"rescaled_N\"]\n","z_input = train_data[\"rescaled_Z\"]\n","\n","#Liquid drop inputs\n","surf_input = train_data[\"rescaled_Surf\"]\n","asym_input = train_data[\"rescaled_Asym\"]\n","coul_input = train_data[\"rescaled_Coul\"]\n","pair_input = train_data[\"rescaled_Pair\"]\n","\n","#Other inputs that may help\n","z_parity_input = train_data[\"rescaled_Z_parity\"]\n","n_parity_input = train_data[\"rescaled_N_parity\"]\n","z_distance_input = train_data[\"rescaled_Z_distance\"]  \n","n_distance_input = train_data[\"rescaled_N_distance\"]\n","S1p_input = train_data[\"rescaled_ame_S1p\"]\n","S1n_input = train_data[\"rescaled_ame_S1n\"]\n","S2p_input = train_data[\"rescaled_ame_S2p\"]\n","S2n_input = train_data[\"rescaled_ame_S2n\"]"]},{"cell_type":"markdown","metadata":{"id":"IZXqaGXlRz-Q"},"source":["##### We create a function that will create the ANN. It takes 3 parameters: the number of inputs, the number of layers and the number of neurons per layer desired."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6mHGp_FRz-R"},"outputs":[],"source":["def create_model(num_inputs, num_layers, num_neurons):\n","    inputs = [keras.layers.Input(shape=(1,)) for i in range(num_inputs)]\n","    merged = keras.layers.Concatenate()(inputs)\n","\n","    dense = merged\n","    for i in range(num_layers):\n","        dense = Dense(num_neurons, activation=\"relu\")(dense)\n","    \n","    output = Dense(1, activation=\"relu\")(dense)\n","    model = keras.models.Model(inputs, output)\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"y1fXG6-DRz-T"},"source":["##### We use this function to create an ANN with 12 inputs, 14 hidden layers with 100 neurons each."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmNvfn6GRz-V"},"outputs":[],"source":["model4 = create_model(12,14,100) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmIqwKxjjWf4"},"outputs":[],"source":["model4.compile(optimizer=Adam(learning_rate=0.001), loss=\"mean_squared_error\")"]},{"cell_type":"markdown","metadata":{"id":"BZoWdb-7Rz-X"},"source":["##### We create a class that will allow us to stop the training of the model when a specific loss is reach. Otherwise, the training will continue until the last epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCpjotIyRz-X"},"outputs":[],"source":["class EarlyStoppingByLossValue(Callback):\n","    def __init__(self, value=0.00000009):\n","        self.value = value\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        current_loss = logs.get(\"loss\")\n","        if current_loss < self.value:\n","            self.model.stop_training = True\n","            print(\"Early stopping by loss value at epoch\", epoch)\n","\n","early_stopping = EarlyStoppingByLossValue()"]},{"cell_type":"markdown","metadata":{"id":"rUhu_EEuRz-Y"},"source":["##### We train the model and plot the evolution of the loss among epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLFgPppjjWf6"},"outputs":[],"source":["history4=model4.fit(x=([n_input, z_input, surf_input, coul_input, asym_input,\n","                        pair_input, S1n_input, S1p_input, S2n_input, S2p_input, \n","                        z_distance_input, n_distance_input]),\n","                        y=target, epochs=2000, shuffle=True,\n","                        verbose=2, callbacks=[early_stopping] )\n","\n","plt.figure(figsize =(20,13))\n","plt.yscale('log')\n","plt.legend('labels')\n","\n","plt.plot(history4.history[\"loss\"])\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"loss\")\n","plt.show()\n","#Be careful : loss is mean_squared_error, not RMSE"]},{"cell_type":"markdown","metadata":{"id":"v5_NqNKtRz-Z"},"source":["##### We rescale the target data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TJ8maWExDrn"},"outputs":[],"source":["scaler = MinMaxScaler(feature_range=(0,1))\n","\n","rescaled_data = pd.read_csv(join(project_path,\"rescaled_data/rescaled_data.csv\"), sep=\";\")\n","\n","rescaled_target = scaler.fit_transform(pd.Series.to_numpy(rescaled_data[\"ame_BE\"]).reshape(-1,1))"]},{"cell_type":"markdown","metadata":{"id":"xT_hnY0dRz-a"},"source":["#### Here, we predict the binding energy for all the nuclei used for training, and we compute the difference between the prediction and experimental data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gc0-j1dTjWgD"},"outputs":[],"source":["train_predictions = model4.predict(x=([n_input, z_input, surf_input, coul_input,\n","                                       asym_input, pair_input, S1n_input, S1p_input,  \n","                                       S2n_input, S2p_input, z_distance_input,\n","                                       n_distance_input]), verbose=0)\n","\n","\n","train_rescaled_predictions = [(i - scaler.min_)/scaler.scale_ for i in train_predictions]\n","\n","\n","train_data[\"BE_Predictions\"] = np.double(train_rescaled_predictions)\n","train_data[\"Difference_BE_AME_ANN\"] = train_data[\"ame_BE\"] - train_data[\"BE_Predictions\"]\n","train_data[\"Difference_BE_DZ_AME\"] = train_data[\"dz_BE\"] - train_data[\"ame_BE\"]"]},{"cell_type":"markdown","metadata":{"id":"nc597XCERz-a"},"source":["##### We will now do the same operation as above, but we will use the model to predict binding energies for the nuclei it has never trained on (Z=10,38,54,68,82) (validation data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeFQ3SxWjWf-"},"outputs":[],"source":["validation_data = pd.read_csv(join(project_path,\"rescaled_data/validation_rescaled_data.csv\"), sep=\";\")\n","\n","test_target = validation_data[\"rescaled_ame_BE\"]\n","test_n_input = validation_data[\"rescaled_N\"]\n","test_z_input = validation_data[\"rescaled_Z\"]\n","test_coul_input = validation_data[\"rescaled_Coul\"]\n","test_surf_input = validation_data[\"rescaled_Surf\"]\n","test_asym_input = validation_data[\"rescaled_Asym\"]\n","test_pair_input = validation_data[\"rescaled_Pair\"]\n","test_z_parity_input = validation_data[\"rescaled_Z_parity\"]\n","test_n_parity_input = validation_data[\"rescaled_N_parity\"]\n","test_z_distance_input = validation_data[\"rescaled_Z_distance\"]\n","test_n_distance_input = validation_data[\"rescaled_N_distance\"]\n","test_S1p_input = validation_data[\"rescaled_ame_S1p\"]\n","test_S1n_input = validation_data[\"rescaled_ame_S1n\"]\n","test_S2p_input = validation_data[\"rescaled_ame_S2p\"]\n","test_S2n_input = validation_data[\"rescaled_ame_S2n\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-H678VkjWgB"},"outputs":[],"source":["validation_predictions = model4.predict(x=([test_n_input, test_z_input,\n","                                            test_surf_input, test_coul_input,\n","                                            test_asym_input, test_pair_input,  \n","                                            test_S1n_input, test_S1p_input,\n","                                            test_S2n_input, test_S2p_input,\n","                                            test_z_distance_input,\n","                                            test_n_distance_input]))\n","\n","validation_rescaled_predictions = [ (i - scaler.min_)/scaler.scale_ for i in validation_predictions]\n","\n","validation_data[\"BE_Predictions\"] = np.double(validation_rescaled_predictions)\n","validation_data[\"Difference_BE_AME_ANN\"] = validation_data[\"ame_BE\"] - validation_data[\"BE_Predictions\"]\n"]},{"cell_type":"markdown","metadata":{"id":"gJOQ3-L8Rz-b"},"source":["##### Here, we compute the values for Sn, Sp, S2n et S2p with the predicted binding energies for both datasets (training and validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQIeYPLhRz-c"},"outputs":[],"source":["train_data=train_data.sort_values(by=['A','N'], ascending=True)\n","train_data.head(15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vinlI700Rz-c"},"outputs":[],"source":["train_data=train_data.sort_values(by=['N','Z'], ascending=True)\n","train_data['Prediction_S2p'] = train_data['BE_Predictions'] - train_data['BE_Predictions'].shift(2)\n","\n","train_data = train_data.sort_values(by=['A','N'], ascending=True)\n","train_data['Prediction_S2n'] = train_data['BE_Predictions'] - train_data['BE_Predictions'].shift(2)\n","train_data['Prediction_S1n'] = train_data['BE_Predictions'] - train_data['BE_Predictions'].shift(1)\n","\n","train_data[\"Difference_S2n_AME_Predictions\"] = train_data[\"ame_S2n\"] - train_data[\"Prediction_S2n\"]\n","train_data[\"Difference_S2p_AME_Predictions\"] = train_data[\"ame_S2p\"] - train_data[\"Prediction_S2p\"]\n","\n","train_data[\"Difference_S2n_DZ_Predictions\"] = train_data[\"dz_S2n\"] - train_data[\"Prediction_S2n\"]\n","train_data[\"Difference_S2p_DZ_Predictions\"] = train_data[\"dz_S2p\"] - train_data[\"Prediction_S2p\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7NRMngSRz-c"},"outputs":[],"source":["validation_data.sort_values(by=['N','Z'], ascending=True)\n","validation_data['Prediction_S2p'] = validation_data['BE_Predictions'] - validation_data['BE_Predictions'].shift(2)\n","\n","validation_data = validation_data.sort_values(by=['A','N'], ascending=True)\n","validation_data['Prediction_S2n'] = validation_data['BE_Predictions'] - validation_data['BE_Predictions'].shift(2)\n","validation_data['Prediction_S1n'] = validation_data['BE_Predictions'] - validation_data['BE_Predictions'].shift(1)\n","\n","validation_data[\"Difference_S2n_AME_Predictions\"] = validation_data[\"ame_S2n\"] - validation_data[\"Prediction_S2n\"]\n","validation_data[\"Difference_S2p_AME_Predictions\"] = validation_data[\"ame_S2p\"] - validation_data[\"Prediction_S2p\"]\n","\n","validation_data[\"Difference_S2n_DZ_Predictions\"] = validation_data[\"dz_S2n\"] - validation_data[\"Prediction_S2n\"]\n","validation_data[\"Difference_S2p_DZ_Predictions\"] = validation_data[\"dz_S2p\"] - validation_data[\"Prediction_S2p\"]"]},{"cell_type":"markdown","metadata":{"id":"QYFT4_aK0Dhv"},"source":["#### We save the predictions into .csv and we will plot them on another notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPAdnnoxrpyL"},"outputs":[],"source":["train_final_csv = train_data.to_csv(join(project_path,\"final_data/train_final_data.csv\"),sep=\";\")\n","validation_final_csv = validation_data.to_csv(join(project_path,\"final_data/validation_final_data.csv\"),sep=\";\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"c74ce178243a7b9e0e299562590941cc463e41c75e32712c41b5e2a834924d12"}}},"nbformat":4,"nbformat_minor":0}