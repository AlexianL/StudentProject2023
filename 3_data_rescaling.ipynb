{"cells":[{"cell_type":"markdown","source":["## Data rescaling \n","----\n","Artificial neural networks work best when dealing with small value data. Some of our inputs in the ANN will be the proton and neutron numbers, which can go to values up to 200. This is why we need to rescale them, and we will do so in this notebook by rescaling them between 0 and 1. We will then separate the data into two different datasets (and thus two different .csv file) to create a training dataset and a test dataset for our ANN. It is needed to save also the data which has not been separated to be able to rescale back the values in the following notebooks. "],"metadata":{"id":"dyHgCojnOChF"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"nxmZH6oQkSKi","executionInfo":{"status":"ok","timestamp":1676457460421,"user_tz":-60,"elapsed":1507,"user":{"displayName":"Alexian LEJEUNE","userId":"07634551298599753712"}}},"outputs":[],"source":["#Libraries for data processing\n","import numpy as np \n","import pandas as pd\n","\n","#Libraries for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns \n","sns.set(color_codes = True)\n","sns.set(font_scale=1.5) #fixing font size\n","\n","#Library for rescaling\n","from sklearn.preprocessing import MinMaxScaler"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"orDulxoulS1n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676457557893,"user_tz":-60,"elapsed":88469,"user":{"displayName":"Alexian LEJEUNE","userId":"07634551298599753712"}},"outputId":"97e4254f-3dcb-41dc-db7c-9c29545cdd39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from logging import RootLogger\n","#Mount Google Drive\n","from google.colab import drive #import drive from google colab\n","\n","root = \"/content/drive\"     #default location for the drive\n","\n","drive.mount(root)           #we mount the google drive at /content/drive\n","\n","#import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n","from os.path import join  \n","\n","#path to your project on Google Drive\n","my_google_drive_path = \"MyDrive/StudentProject2023-main\"\n","\n","project_path = join(root, my_google_drive_path)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"TIsDRCCMoa6b","executionInfo":{"status":"ok","timestamp":1676457576254,"user_tz":-60,"elapsed":581,"user":{"displayName":"Alexian LEJEUNE","userId":"07634551298599753712"}}},"outputs":[],"source":["merged_data = pd.read_csv(join(project_path,\"2_processed_data/merged_data.csv\"), sep=\";\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"yjoGRPPBovyx","executionInfo":{"status":"ok","timestamp":1676457576255,"user_tz":-60,"elapsed":8,"user":{"displayName":"Alexian LEJEUNE","userId":"07634551298599753712"}}},"outputs":[],"source":["scaler = MinMaxScaler(feature_range=(0,1))\n","\n","def rescale(list) :\n","    \"\"\"This function adds new columns to the merged dataframe with data rescaled\n","    between 0 and 1\"\"\"\n","    for column in list :\n","        merged_data[\"rescaled_\"+column]=scaler.fit_transform(pd.Series.to_numpy(merged_data[column]).reshape(-1,1))"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"yXwhyo72tbYJ","executionInfo":{"status":"ok","timestamp":1676457576476,"user_tz":-60,"elapsed":7,"user":{"displayName":"Alexian LEJEUNE","userId":"07634551298599753712"}}},"outputs":[],"source":["columns=[\"ame_BE\",\"N\",\"Z\",\"Surf\",\"Asym\",\"Coul\",\"Pair\",\"Z_parity\",\"N_parity\",\"Z_distance\",\"N_distance\", \"ame_S1p\", \"ame_S1n\", \"ame_S2p\", \"ame_S2n\"]\n","\n","rescale(columns)"]},{"cell_type":"markdown","source":["###We save this merged dataframe to .csv format"],"metadata":{"id":"tkk3PNaq_rjj"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"waJIa9BQt-8n","executionInfo":{"status":"ok","timestamp":1676457589828,"user_tz":-60,"elapsed":273,"user":{"displayName":"Alexian LEJEUNE","userId":"07634551298599753712"}}},"outputs":[],"source":["merged_data.to_csv(join(project_path,\"3_rescaled_data/rescaled_data.csv\"),sep=\";\", index=False)"]},{"cell_type":"markdown","source":["###Before separating the previous merged dataframe into a training data and a test dataset, we will get rid of some nuclei. "],"metadata":{"id":"Kwlfre4r_0zW"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"zIqgHAJ7O9e2","executionInfo":{"status":"ok","timestamp":1676457623694,"user_tz":-60,"elapsed":207,"user":{"displayName":"Alexian LEJEUNE","userId":"07634551298599753712"}}},"outputs":[],"source":["merged_data.drop(merged_data[(merged_data[\"ame_S2n\"]<0 )].index, inplace=True)\n","merged_data.drop(merged_data[(merged_data[\"ame_S2p\"]<0 )].index, inplace=True)\n","\n","merged_data.drop(merged_data[(merged_data[\"ame_S1n\"]<0 )].index, inplace=True)\n","merged_data.drop(merged_data[(merged_data[\"ame_S1p\"]<0 )].index, inplace=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"6evvx9njuZSH","executionInfo":{"status":"ok","timestamp":1676457648341,"user_tz":-60,"elapsed":5016,"user":{"displayName":"Alexian LEJEUNE","userId":"07634551298599753712"}}},"outputs":[],"source":["#From the merged table, create one training dataset and a test dataset\n","#Not sure the next two lines are useful\n","train_data = pd.DataFrame(columns=[\"Z\",\"N\",\"dz_BE/A\",\"dz_ME\",\"A\",\"dz_BE\",\"dz_S1n\",\"dz_S1p\",\"dz_S2p\", \"dz_S2n\",\"ame_ME\", \"ame_BE/A\", \"ame_AM\", \"ame_BE\", \"ame_S1p\", \"ame_S1n\", \"ame_S2p\", \"ame_S2n\", \"BE_diff_dz_ame\",\"Surf\",\"Asym\",\"Coul\",\"Pair\",\"Z_parity\",\"N_parity\",\"Z_distance\",\"N_distance\"])\n","test_data = pd.DataFrame(columns=[\"Z\",\"N\",\"dz_BE/A\",\"dz_ME\",\"A\",\"dz_BE\",\"dz_S1n\",\"dz_S1p\",\"dz_S2p\", \"dz_S2n\",\"ame_ME\", \"ame_BE/A\", \"ame_AM\", \"ame_BE\", \"ame_S1p\", \"ame_S1n\", \"ame_S2p\", \"ame_S2n\", \"BE_diff_dz_ame\",\"Surf\",\"Asym\",\"Coul\",\"Pair\",\"Z_parity\",\"N_parity\",\"Z_distance\",\"N_distance\"])\n","\n","\n","#We separate the merged dataframe into training and test datasets\n","for i in range(len(merged_data)) :\n","    \n","    if int(merged_data.iloc[i][\"Z\"]) in [10,38,54,68,82] :\n","        test_data = test_data.append(merged_data.iloc[i], ignore_index=True)\n","\n","    else :\n","        train_data = train_data.append(merged_data.iloc[i], ignore_index=True)\n","\n","\n","#We don't use training data with A<16 because these light nuclei experience\n","#Physics phenomenon that are very far from trivial (halo etc)\n","train_data.drop(train_data[(train_data[\"A\"]<16 )].index, inplace=True)\n","train_data.drop(train_data[(train_data[\"ame_S2n\"]<0 )].index, inplace=True)\n","train_data.drop(train_data[(train_data[\"ame_S2p\"]<0 )].index, inplace=True)\n","test_data.drop(test_data[(test_data[\"ame_S2n\"]<0 )].index, inplace=True)\n","test_data.drop(test_data[(test_data[\"ame_S2p\"]<0 )].index, inplace=True)\n","\n","train_data.drop(train_data[(train_data[\"ame_S1n\"]<0 )].index, inplace=True)\n","train_data.drop(train_data[(train_data[\"ame_S1p\"]<0 )].index, inplace=True)\n","test_data.drop(test_data[(test_data[\"ame_S1n\"]<0 )].index, inplace=True)\n","test_data.drop(test_data[(test_data[\"ame_S1p\"]<0 )].index, inplace=True)\n","\n","train_merged_csv = train_data.to_csv(join(project_path,\"3_rescaled_data/train_rescaled_data.csv\"),sep=\";\")\n","test_merged_csv = test_data.to_csv(join(project_path,\"3_rescaled_data/test_rescaled_data.csv\"),sep=\";\")"]},{"cell_type":"code","source":[],"metadata":{"id":"N8ReQN7lqYsu"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"c74ce178243a7b9e0e299562590941cc463e41c75e32712c41b5e2a834924d12"}}},"nbformat":4,"nbformat_minor":0}